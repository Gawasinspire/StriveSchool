{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laughing-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simplified-cooperative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6279, 2.2870, 0.9885], grad_fn=<AddBackward0>) <AddBackward0 object at 0x7fd5340b81d0>\n",
      "tensor([ 7.9504, 15.6909,  2.9312], grad_fn=<MulBackward0>)\n",
      "tensor(8.8575, grad_fn=<MeanBackward0>)\n",
      "tensor([3.2558, 4.5740, 1.9769])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad =True)\n",
    "x\n",
    "y = x+2\n",
    "print(y,y.grad_fn)\n",
    "z = y*y*3\n",
    "print(z)\n",
    "#z.backward()\n",
    "z = z.mean()\n",
    "print(z)\n",
    "#now it workd\n",
    "z.backward()\n",
    "print(x.grad)  # dz/dx --> local gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tested-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad =True)\n",
    "y = x*2\n",
    "for _ in range(10):\n",
    "    y = y*2\n",
    "print(y.shape)\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "handmade-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x7f574ef49990>\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# -------------\n",
    "# Stop a tensor from tracking history:\n",
    "# For example during our training loop when we want to update our weights\n",
    "# then this update operation should not be part of the gradient computation\n",
    "# - x.requires_grad_(False)\n",
    "# - x.detach()\n",
    "# - wrap in 'with torch.no_grad():'\n",
    "\n",
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)\n",
    "\n",
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b = a.detach()\n",
    "print(b.requires_grad)\n",
    "\n",
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-convention",
   "metadata": {},
   "source": [
    "### backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "radio-stone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -------------\n",
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "print(weights)\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-pitch",
   "metadata": {},
   "source": [
    "# forwardpass gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "simplified-triple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-wells",
   "metadata": {},
   "source": [
    "## gradients using backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifty-oliver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True)\n",
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.440, loss= 44.0000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# need 2 torch\n",
    "#X = torch.tensor([1,2,5,6,8], dtype = torch.float32)\n",
    "#Y = torch.tensor([2,4,6,7,9], dtype = torch.float32)\n",
    "X = torch.tensor([1,2,3,4,5], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8,10], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "print(w)\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(6).item():.3f}')\n",
    "\n",
    "# initial training\n",
    "learning_rate = 0.01\n",
    "steps = 100\n",
    "\n",
    "for epoch in range(steps):\n",
    "    # pred\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss actual - predicted\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    # gradient now backwards\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    #zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "    #print(dir(w.grad))\n",
    "    if epoch % 100 == 0:\n",
    "        #print(y_pred)\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss= {l.item():.4f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "antique-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n",
      "tensor([2.0000, 4.0000, 6.0000, 8.0000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-duration",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "necessary-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch  1 : w =  tensor(0.0010, requires_grad=True)  loss =  tensor(30., grad_fn=<MeanBackward0>)\n",
      "epoch  11 : w =  tensor(0.0110, requires_grad=True)  loss =  tensor(29.7008, grad_fn=<MeanBackward0>)\n",
      "epoch  21 : w =  tensor(0.0210, requires_grad=True)  loss =  tensor(29.4034, grad_fn=<MeanBackward0>)\n",
      "epoch  31 : w =  tensor(0.0310, requires_grad=True)  loss =  tensor(29.1079, grad_fn=<MeanBackward0>)\n",
      "epoch  41 : w =  tensor(0.0409, requires_grad=True)  loss =  tensor(28.8144, grad_fn=<MeanBackward0>)\n",
      "epoch  51 : w =  tensor(0.0508, requires_grad=True)  loss =  tensor(28.5232, grad_fn=<MeanBackward0>)\n",
      "epoch  61 : w =  tensor(0.0607, requires_grad=True)  loss =  tensor(28.2341, grad_fn=<MeanBackward0>)\n",
      "epoch  71 : w =  tensor(0.0706, requires_grad=True)  loss =  tensor(27.9472, grad_fn=<MeanBackward0>)\n",
      "epoch  81 : w =  tensor(0.0805, requires_grad=True)  loss =  tensor(27.6624, grad_fn=<MeanBackward0>)\n",
      "epoch  91 : w =  tensor(0.0903, requires_grad=True)  loss =  tensor(27.3799, grad_fn=<MeanBackward0>)\n",
      "epoch  101 : w =  tensor(0.1001, requires_grad=True)  loss =  tensor(27.0995, grad_fn=<MeanBackward0>)\n",
      "epoch  111 : w =  tensor(0.1099, requires_grad=True)  loss =  tensor(26.8212, grad_fn=<MeanBackward0>)\n",
      "epoch  121 : w =  tensor(0.1197, requires_grad=True)  loss =  tensor(26.5451, grad_fn=<MeanBackward0>)\n",
      "epoch  131 : w =  tensor(0.1294, requires_grad=True)  loss =  tensor(26.2711, grad_fn=<MeanBackward0>)\n",
      "epoch  141 : w =  tensor(0.1391, requires_grad=True)  loss =  tensor(25.9993, grad_fn=<MeanBackward0>)\n",
      "epoch  151 : w =  tensor(0.1488, requires_grad=True)  loss =  tensor(25.7295, grad_fn=<MeanBackward0>)\n",
      "epoch  161 : w =  tensor(0.1584, requires_grad=True)  loss =  tensor(25.4618, grad_fn=<MeanBackward0>)\n",
      "epoch  171 : w =  tensor(0.1681, requires_grad=True)  loss =  tensor(25.1962, grad_fn=<MeanBackward0>)\n",
      "epoch  181 : w =  tensor(0.1777, requires_grad=True)  loss =  tensor(24.9327, grad_fn=<MeanBackward0>)\n",
      "epoch  191 : w =  tensor(0.1873, requires_grad=True)  loss =  tensor(24.6713, grad_fn=<MeanBackward0>)\n",
      "epoch  201 : w =  tensor(0.1968, requires_grad=True)  loss =  tensor(24.4118, grad_fn=<MeanBackward0>)\n",
      "epoch  211 : w =  tensor(0.2064, requires_grad=True)  loss =  tensor(24.1545, grad_fn=<MeanBackward0>)\n",
      "epoch  221 : w =  tensor(0.2159, requires_grad=True)  loss =  tensor(23.8991, grad_fn=<MeanBackward0>)\n",
      "epoch  231 : w =  tensor(0.2253, requires_grad=True)  loss =  tensor(23.6458, grad_fn=<MeanBackward0>)\n",
      "epoch  241 : w =  tensor(0.2348, requires_grad=True)  loss =  tensor(23.3945, grad_fn=<MeanBackward0>)\n",
      "epoch  251 : w =  tensor(0.2442, requires_grad=True)  loss =  tensor(23.1452, grad_fn=<MeanBackward0>)\n",
      "epoch  261 : w =  tensor(0.2536, requires_grad=True)  loss =  tensor(22.8978, grad_fn=<MeanBackward0>)\n",
      "epoch  271 : w =  tensor(0.2630, requires_grad=True)  loss =  tensor(22.6524, grad_fn=<MeanBackward0>)\n",
      "epoch  281 : w =  tensor(0.2724, requires_grad=True)  loss =  tensor(22.4090, grad_fn=<MeanBackward0>)\n",
      "epoch  291 : w =  tensor(0.2817, requires_grad=True)  loss =  tensor(22.1676, grad_fn=<MeanBackward0>)\n",
      "epoch  301 : w =  tensor(0.2910, requires_grad=True)  loss =  tensor(21.9281, grad_fn=<MeanBackward0>)\n",
      "epoch  311 : w =  tensor(0.3003, requires_grad=True)  loss =  tensor(21.6905, grad_fn=<MeanBackward0>)\n",
      "epoch  321 : w =  tensor(0.3096, requires_grad=True)  loss =  tensor(21.4548, grad_fn=<MeanBackward0>)\n",
      "epoch  331 : w =  tensor(0.3188, requires_grad=True)  loss =  tensor(21.2211, grad_fn=<MeanBackward0>)\n",
      "epoch  341 : w =  tensor(0.3280, requires_grad=True)  loss =  tensor(20.9892, grad_fn=<MeanBackward0>)\n",
      "epoch  351 : w =  tensor(0.3372, requires_grad=True)  loss =  tensor(20.7592, grad_fn=<MeanBackward0>)\n",
      "epoch  361 : w =  tensor(0.3464, requires_grad=True)  loss =  tensor(20.5312, grad_fn=<MeanBackward0>)\n",
      "epoch  371 : w =  tensor(0.3555, requires_grad=True)  loss =  tensor(20.3049, grad_fn=<MeanBackward0>)\n",
      "epoch  381 : w =  tensor(0.3646, requires_grad=True)  loss =  tensor(20.0806, grad_fn=<MeanBackward0>)\n",
      "epoch  391 : w =  tensor(0.3737, requires_grad=True)  loss =  tensor(19.8581, grad_fn=<MeanBackward0>)\n",
      "epoch  401 : w =  tensor(0.3828, requires_grad=True)  loss =  tensor(19.6374, grad_fn=<MeanBackward0>)\n",
      "epoch  411 : w =  tensor(0.3918, requires_grad=True)  loss =  tensor(19.4185, grad_fn=<MeanBackward0>)\n",
      "epoch  421 : w =  tensor(0.4008, requires_grad=True)  loss =  tensor(19.2015, grad_fn=<MeanBackward0>)\n",
      "epoch  431 : w =  tensor(0.4098, requires_grad=True)  loss =  tensor(18.9863, grad_fn=<MeanBackward0>)\n",
      "epoch  441 : w =  tensor(0.4188, requires_grad=True)  loss =  tensor(18.7728, grad_fn=<MeanBackward0>)\n",
      "epoch  451 : w =  tensor(0.4277, requires_grad=True)  loss =  tensor(18.5612, grad_fn=<MeanBackward0>)\n",
      "epoch  461 : w =  tensor(0.4367, requires_grad=True)  loss =  tensor(18.3513, grad_fn=<MeanBackward0>)\n",
      "epoch  471 : w =  tensor(0.4455, requires_grad=True)  loss =  tensor(18.1432, grad_fn=<MeanBackward0>)\n",
      "epoch  481 : w =  tensor(0.4544, requires_grad=True)  loss =  tensor(17.9369, grad_fn=<MeanBackward0>)\n",
      "epoch  491 : w =  tensor(0.4633, requires_grad=True)  loss =  tensor(17.7323, grad_fn=<MeanBackward0>)\n",
      "epoch  501 : w =  tensor(0.4721, requires_grad=True)  loss =  tensor(17.5294, grad_fn=<MeanBackward0>)\n",
      "epoch  511 : w =  tensor(0.4809, requires_grad=True)  loss =  tensor(17.3283, grad_fn=<MeanBackward0>)\n",
      "epoch  521 : w =  tensor(0.4896, requires_grad=True)  loss =  tensor(17.1289, grad_fn=<MeanBackward0>)\n",
      "epoch  531 : w =  tensor(0.4984, requires_grad=True)  loss =  tensor(16.9312, grad_fn=<MeanBackward0>)\n",
      "epoch  541 : w =  tensor(0.5071, requires_grad=True)  loss =  tensor(16.7352, grad_fn=<MeanBackward0>)\n",
      "epoch  551 : w =  tensor(0.5158, requires_grad=True)  loss =  tensor(16.5409, grad_fn=<MeanBackward0>)\n",
      "epoch  561 : w =  tensor(0.5245, requires_grad=True)  loss =  tensor(16.3482, grad_fn=<MeanBackward0>)\n",
      "epoch  571 : w =  tensor(0.5331, requires_grad=True)  loss =  tensor(16.1573, grad_fn=<MeanBackward0>)\n",
      "epoch  581 : w =  tensor(0.5417, requires_grad=True)  loss =  tensor(15.9680, grad_fn=<MeanBackward0>)\n",
      "epoch  591 : w =  tensor(0.5503, requires_grad=True)  loss =  tensor(15.7803, grad_fn=<MeanBackward0>)\n",
      "epoch  601 : w =  tensor(0.5589, requires_grad=True)  loss =  tensor(15.5943, grad_fn=<MeanBackward0>)\n",
      "epoch  611 : w =  tensor(0.5674, requires_grad=True)  loss =  tensor(15.4099, grad_fn=<MeanBackward0>)\n",
      "epoch  621 : w =  tensor(0.5760, requires_grad=True)  loss =  tensor(15.2271, grad_fn=<MeanBackward0>)\n",
      "epoch  631 : w =  tensor(0.5845, requires_grad=True)  loss =  tensor(15.0460, grad_fn=<MeanBackward0>)\n",
      "epoch  641 : w =  tensor(0.5929, requires_grad=True)  loss =  tensor(14.8664, grad_fn=<MeanBackward0>)\n",
      "epoch  651 : w =  tensor(0.6014, requires_grad=True)  loss =  tensor(14.6885, grad_fn=<MeanBackward0>)\n",
      "epoch  661 : w =  tensor(0.6098, requires_grad=True)  loss =  tensor(14.5121, grad_fn=<MeanBackward0>)\n",
      "epoch  671 : w =  tensor(0.6182, requires_grad=True)  loss =  tensor(14.3373, grad_fn=<MeanBackward0>)\n",
      "epoch  681 : w =  tensor(0.6266, requires_grad=True)  loss =  tensor(14.1641, grad_fn=<MeanBackward0>)\n",
      "epoch  691 : w =  tensor(0.6349, requires_grad=True)  loss =  tensor(13.9925, grad_fn=<MeanBackward0>)\n",
      "epoch  701 : w =  tensor(0.6433, requires_grad=True)  loss =  tensor(13.8224, grad_fn=<MeanBackward0>)\n",
      "epoch  711 : w =  tensor(0.6516, requires_grad=True)  loss =  tensor(13.6538, grad_fn=<MeanBackward0>)\n",
      "epoch  721 : w =  tensor(0.6598, requires_grad=True)  loss =  tensor(13.4867, grad_fn=<MeanBackward0>)\n",
      "epoch  731 : w =  tensor(0.6681, requires_grad=True)  loss =  tensor(13.3212, grad_fn=<MeanBackward0>)\n",
      "epoch  741 : w =  tensor(0.6763, requires_grad=True)  loss =  tensor(13.1572, grad_fn=<MeanBackward0>)\n",
      "epoch  751 : w =  tensor(0.6845, requires_grad=True)  loss =  tensor(12.9947, grad_fn=<MeanBackward0>)\n",
      "epoch  761 : w =  tensor(0.6927, requires_grad=True)  loss =  tensor(12.8337, grad_fn=<MeanBackward0>)\n",
      "epoch  771 : w =  tensor(0.7009, requires_grad=True)  loss =  tensor(12.6742, grad_fn=<MeanBackward0>)\n",
      "epoch  781 : w =  tensor(0.7090, requires_grad=True)  loss =  tensor(12.5161, grad_fn=<MeanBackward0>)\n",
      "epoch  791 : w =  tensor(0.7171, requires_grad=True)  loss =  tensor(12.3595, grad_fn=<MeanBackward0>)\n",
      "epoch  801 : w =  tensor(0.7252, requires_grad=True)  loss =  tensor(12.2044, grad_fn=<MeanBackward0>)\n",
      "epoch  811 : w =  tensor(0.7332, requires_grad=True)  loss =  tensor(12.0507, grad_fn=<MeanBackward0>)\n",
      "epoch  821 : w =  tensor(0.7413, requires_grad=True)  loss =  tensor(11.8985, grad_fn=<MeanBackward0>)\n",
      "epoch  831 : w =  tensor(0.7493, requires_grad=True)  loss =  tensor(11.7477, grad_fn=<MeanBackward0>)\n",
      "epoch  841 : w =  tensor(0.7572, requires_grad=True)  loss =  tensor(11.5983, grad_fn=<MeanBackward0>)\n",
      "epoch  851 : w =  tensor(0.7652, requires_grad=True)  loss =  tensor(11.4504, grad_fn=<MeanBackward0>)\n",
      "epoch  861 : w =  tensor(0.7731, requires_grad=True)  loss =  tensor(11.3038, grad_fn=<MeanBackward0>)\n",
      "epoch  871 : w =  tensor(0.7810, requires_grad=True)  loss =  tensor(11.1587, grad_fn=<MeanBackward0>)\n",
      "epoch  881 : w =  tensor(0.7889, requires_grad=True)  loss =  tensor(11.0149, grad_fn=<MeanBackward0>)\n",
      "epoch  891 : w =  tensor(0.7968, requires_grad=True)  loss =  tensor(10.8725, grad_fn=<MeanBackward0>)\n",
      "epoch  901 : w =  tensor(0.8046, requires_grad=True)  loss =  tensor(10.7315, grad_fn=<MeanBackward0>)\n",
      "epoch  911 : w =  tensor(0.8124, requires_grad=True)  loss =  tensor(10.5919, grad_fn=<MeanBackward0>)\n",
      "epoch  921 : w =  tensor(0.8202, requires_grad=True)  loss =  tensor(10.4536, grad_fn=<MeanBackward0>)\n",
      "epoch  931 : w =  tensor(0.8279, requires_grad=True)  loss =  tensor(10.3166, grad_fn=<MeanBackward0>)\n",
      "epoch  941 : w =  tensor(0.8357, requires_grad=True)  loss =  tensor(10.1810, grad_fn=<MeanBackward0>)\n",
      "epoch  951 : w =  tensor(0.8434, requires_grad=True)  loss =  tensor(10.0467, grad_fn=<MeanBackward0>)\n",
      "epoch  961 : w =  tensor(0.8511, requires_grad=True)  loss =  tensor(9.9138, grad_fn=<MeanBackward0>)\n",
      "epoch  971 : w =  tensor(0.8587, requires_grad=True)  loss =  tensor(9.7821, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  981 : w =  tensor(0.8663, requires_grad=True)  loss =  tensor(9.6518, grad_fn=<MeanBackward0>)\n",
      "epoch  991 : w =  tensor(0.8739, requires_grad=True)  loss =  tensor(9.5228, grad_fn=<MeanBackward0>)\n",
      "epoch  1001 : w =  tensor(0.8815, requires_grad=True)  loss =  tensor(9.3950, grad_fn=<MeanBackward0>)\n",
      "epoch  1011 : w =  tensor(0.8891, requires_grad=True)  loss =  tensor(9.2686, grad_fn=<MeanBackward0>)\n",
      "epoch  1021 : w =  tensor(0.8966, requires_grad=True)  loss =  tensor(9.1434, grad_fn=<MeanBackward0>)\n",
      "epoch  1031 : w =  tensor(0.9041, requires_grad=True)  loss =  tensor(9.0194, grad_fn=<MeanBackward0>)\n",
      "epoch  1041 : w =  tensor(0.9116, requires_grad=True)  loss =  tensor(8.8968, grad_fn=<MeanBackward0>)\n",
      "epoch  1051 : w =  tensor(0.9191, requires_grad=True)  loss =  tensor(8.7753, grad_fn=<MeanBackward0>)\n",
      "epoch  1061 : w =  tensor(0.9265, requires_grad=True)  loss =  tensor(8.6551, grad_fn=<MeanBackward0>)\n",
      "epoch  1071 : w =  tensor(0.9339, requires_grad=True)  loss =  tensor(8.5362, grad_fn=<MeanBackward0>)\n",
      "epoch  1081 : w =  tensor(0.9413, requires_grad=True)  loss =  tensor(8.4184, grad_fn=<MeanBackward0>)\n",
      "epoch  1091 : w =  tensor(0.9486, requires_grad=True)  loss =  tensor(8.3019, grad_fn=<MeanBackward0>)\n",
      "epoch  1101 : w =  tensor(0.9560, requires_grad=True)  loss =  tensor(8.1866, grad_fn=<MeanBackward0>)\n",
      "epoch  1111 : w =  tensor(0.9633, requires_grad=True)  loss =  tensor(8.0725, grad_fn=<MeanBackward0>)\n",
      "epoch  1121 : w =  tensor(0.9705, requires_grad=True)  loss =  tensor(7.9596, grad_fn=<MeanBackward0>)\n",
      "epoch  1131 : w =  tensor(0.9778, requires_grad=True)  loss =  tensor(7.8478, grad_fn=<MeanBackward0>)\n",
      "epoch  1141 : w =  tensor(0.9850, requires_grad=True)  loss =  tensor(7.7373, grad_fn=<MeanBackward0>)\n",
      "epoch  1151 : w =  tensor(0.9922, requires_grad=True)  loss =  tensor(7.6279, grad_fn=<MeanBackward0>)\n",
      "epoch  1161 : w =  tensor(0.9994, requires_grad=True)  loss =  tensor(7.5197, grad_fn=<MeanBackward0>)\n",
      "epoch  1171 : w =  tensor(1.0066, requires_grad=True)  loss =  tensor(7.4126, grad_fn=<MeanBackward0>)\n",
      "epoch  1181 : w =  tensor(1.0137, requires_grad=True)  loss =  tensor(7.3066, grad_fn=<MeanBackward0>)\n",
      "epoch  1191 : w =  tensor(1.0208, requires_grad=True)  loss =  tensor(7.2018, grad_fn=<MeanBackward0>)\n",
      "epoch  1201 : w =  tensor(1.0279, requires_grad=True)  loss =  tensor(7.0982, grad_fn=<MeanBackward0>)\n",
      "epoch  1211 : w =  tensor(1.0349, requires_grad=True)  loss =  tensor(6.9956, grad_fn=<MeanBackward0>)\n",
      "epoch  1221 : w =  tensor(1.0419, requires_grad=True)  loss =  tensor(6.8942, grad_fn=<MeanBackward0>)\n",
      "epoch  1231 : w =  tensor(1.0489, requires_grad=True)  loss =  tensor(6.7938, grad_fn=<MeanBackward0>)\n",
      "epoch  1241 : w =  tensor(1.0559, requires_grad=True)  loss =  tensor(6.6946, grad_fn=<MeanBackward0>)\n",
      "epoch  1251 : w =  tensor(1.0629, requires_grad=True)  loss =  tensor(6.5965, grad_fn=<MeanBackward0>)\n",
      "epoch  1261 : w =  tensor(1.0698, requires_grad=True)  loss =  tensor(6.4994, grad_fn=<MeanBackward0>)\n",
      "epoch  1271 : w =  tensor(1.0767, requires_grad=True)  loss =  tensor(6.4034, grad_fn=<MeanBackward0>)\n",
      "epoch  1281 : w =  tensor(1.0836, requires_grad=True)  loss =  tensor(6.3085, grad_fn=<MeanBackward0>)\n",
      "epoch  1291 : w =  tensor(1.0904, requires_grad=True)  loss =  tensor(6.2146, grad_fn=<MeanBackward0>)\n",
      "epoch  1301 : w =  tensor(1.0972, requires_grad=True)  loss =  tensor(6.1218, grad_fn=<MeanBackward0>)\n",
      "epoch  1311 : w =  tensor(1.1040, requires_grad=True)  loss =  tensor(6.0300, grad_fn=<MeanBackward0>)\n",
      "epoch  1321 : w =  tensor(1.1108, requires_grad=True)  loss =  tensor(5.9393, grad_fn=<MeanBackward0>)\n",
      "epoch  1331 : w =  tensor(1.1175, requires_grad=True)  loss =  tensor(5.8496, grad_fn=<MeanBackward0>)\n",
      "epoch  1341 : w =  tensor(1.1242, requires_grad=True)  loss =  tensor(5.7609, grad_fn=<MeanBackward0>)\n",
      "epoch  1351 : w =  tensor(1.1309, requires_grad=True)  loss =  tensor(5.6733, grad_fn=<MeanBackward0>)\n",
      "epoch  1361 : w =  tensor(1.1376, requires_grad=True)  loss =  tensor(5.5866, grad_fn=<MeanBackward0>)\n",
      "epoch  1371 : w =  tensor(1.1442, requires_grad=True)  loss =  tensor(5.5010, grad_fn=<MeanBackward0>)\n",
      "epoch  1381 : w =  tensor(1.1509, requires_grad=True)  loss =  tensor(5.4163, grad_fn=<MeanBackward0>)\n",
      "epoch  1391 : w =  tensor(1.1574, requires_grad=True)  loss =  tensor(5.3326, grad_fn=<MeanBackward0>)\n",
      "epoch  1401 : w =  tensor(1.1640, requires_grad=True)  loss =  tensor(5.2499, grad_fn=<MeanBackward0>)\n",
      "epoch  1411 : w =  tensor(1.1705, requires_grad=True)  loss =  tensor(5.1682, grad_fn=<MeanBackward0>)\n",
      "epoch  1421 : w =  tensor(1.1770, requires_grad=True)  loss =  tensor(5.0874, grad_fn=<MeanBackward0>)\n",
      "epoch  1431 : w =  tensor(1.1835, requires_grad=True)  loss =  tensor(5.0076, grad_fn=<MeanBackward0>)\n",
      "epoch  1441 : w =  tensor(1.1900, requires_grad=True)  loss =  tensor(4.9287, grad_fn=<MeanBackward0>)\n",
      "epoch  1451 : w =  tensor(1.1964, requires_grad=True)  loss =  tensor(4.8508, grad_fn=<MeanBackward0>)\n",
      "epoch  1461 : w =  tensor(1.2028, requires_grad=True)  loss =  tensor(4.7738, grad_fn=<MeanBackward0>)\n",
      "epoch  1471 : w =  tensor(1.2092, requires_grad=True)  loss =  tensor(4.6978, grad_fn=<MeanBackward0>)\n",
      "epoch  1481 : w =  tensor(1.2156, requires_grad=True)  loss =  tensor(4.6226, grad_fn=<MeanBackward0>)\n",
      "epoch  1491 : w =  tensor(1.2219, requires_grad=True)  loss =  tensor(4.5484, grad_fn=<MeanBackward0>)\n",
      "epoch  1501 : w =  tensor(1.2282, requires_grad=True)  loss =  tensor(4.4751, grad_fn=<MeanBackward0>)\n",
      "epoch  1511 : w =  tensor(1.2345, requires_grad=True)  loss =  tensor(4.4026, grad_fn=<MeanBackward0>)\n",
      "epoch  1521 : w =  tensor(1.2407, requires_grad=True)  loss =  tensor(4.3311, grad_fn=<MeanBackward0>)\n",
      "epoch  1531 : w =  tensor(1.2469, requires_grad=True)  loss =  tensor(4.2604, grad_fn=<MeanBackward0>)\n",
      "epoch  1541 : w =  tensor(1.2531, requires_grad=True)  loss =  tensor(4.1907, grad_fn=<MeanBackward0>)\n",
      "epoch  1551 : w =  tensor(1.2593, requires_grad=True)  loss =  tensor(4.1218, grad_fn=<MeanBackward0>)\n",
      "epoch  1561 : w =  tensor(1.2654, requires_grad=True)  loss =  tensor(4.0537, grad_fn=<MeanBackward0>)\n",
      "epoch  1571 : w =  tensor(1.2715, requires_grad=True)  loss =  tensor(3.9865, grad_fn=<MeanBackward0>)\n",
      "epoch  1581 : w =  tensor(1.2776, requires_grad=True)  loss =  tensor(3.9202, grad_fn=<MeanBackward0>)\n",
      "epoch  1591 : w =  tensor(1.2837, requires_grad=True)  loss =  tensor(3.8547, grad_fn=<MeanBackward0>)\n",
      "epoch  1601 : w =  tensor(1.2897, requires_grad=True)  loss =  tensor(3.7900, grad_fn=<MeanBackward0>)\n",
      "epoch  1611 : w =  tensor(1.2957, requires_grad=True)  loss =  tensor(3.7262, grad_fn=<MeanBackward0>)\n",
      "epoch  1621 : w =  tensor(1.3017, requires_grad=True)  loss =  tensor(3.6632, grad_fn=<MeanBackward0>)\n",
      "epoch  1631 : w =  tensor(1.3077, requires_grad=True)  loss =  tensor(3.6010, grad_fn=<MeanBackward0>)\n",
      "epoch  1641 : w =  tensor(1.3136, requires_grad=True)  loss =  tensor(3.5396, grad_fn=<MeanBackward0>)\n",
      "epoch  1651 : w =  tensor(1.3195, requires_grad=True)  loss =  tensor(3.4790, grad_fn=<MeanBackward0>)\n",
      "epoch  1661 : w =  tensor(1.3254, requires_grad=True)  loss =  tensor(3.4192, grad_fn=<MeanBackward0>)\n",
      "epoch  1671 : w =  tensor(1.3312, requires_grad=True)  loss =  tensor(3.3602, grad_fn=<MeanBackward0>)\n",
      "epoch  1681 : w =  tensor(1.3371, requires_grad=True)  loss =  tensor(3.3020, grad_fn=<MeanBackward0>)\n",
      "epoch  1691 : w =  tensor(1.3429, requires_grad=True)  loss =  tensor(3.2445, grad_fn=<MeanBackward0>)\n",
      "epoch  1701 : w =  tensor(1.3486, requires_grad=True)  loss =  tensor(3.1879, grad_fn=<MeanBackward0>)\n",
      "epoch  1711 : w =  tensor(1.3544, requires_grad=True)  loss =  tensor(3.1319, grad_fn=<MeanBackward0>)\n",
      "epoch  1721 : w =  tensor(1.3601, requires_grad=True)  loss =  tensor(3.0768, grad_fn=<MeanBackward0>)\n",
      "epoch  1731 : w =  tensor(1.3658, requires_grad=True)  loss =  tensor(3.0223, grad_fn=<MeanBackward0>)\n",
      "epoch  1741 : w =  tensor(1.3714, requires_grad=True)  loss =  tensor(2.9686, grad_fn=<MeanBackward0>)\n",
      "epoch  1751 : w =  tensor(1.3771, requires_grad=True)  loss =  tensor(2.9157, grad_fn=<MeanBackward0>)\n",
      "epoch  1761 : w =  tensor(1.3827, requires_grad=True)  loss =  tensor(2.8635, grad_fn=<MeanBackward0>)\n",
      "epoch  1771 : w =  tensor(1.3882, requires_grad=True)  loss =  tensor(2.8120, grad_fn=<MeanBackward0>)\n",
      "epoch  1781 : w =  tensor(1.3938, requires_grad=True)  loss =  tensor(2.7612, grad_fn=<MeanBackward0>)\n",
      "epoch  1791 : w =  tensor(1.3993, requires_grad=True)  loss =  tensor(2.7111, grad_fn=<MeanBackward0>)\n",
      "epoch  1801 : w =  tensor(1.4048, requires_grad=True)  loss =  tensor(2.6617, grad_fn=<MeanBackward0>)\n",
      "epoch  1811 : w =  tensor(1.4103, requires_grad=True)  loss =  tensor(2.6130, grad_fn=<MeanBackward0>)\n",
      "epoch  1821 : w =  tensor(1.4157, requires_grad=True)  loss =  tensor(2.5650, grad_fn=<MeanBackward0>)\n",
      "epoch  1831 : w =  tensor(1.4211, requires_grad=True)  loss =  tensor(2.5177, grad_fn=<MeanBackward0>)\n",
      "epoch  1841 : w =  tensor(1.4265, requires_grad=True)  loss =  tensor(2.4711, grad_fn=<MeanBackward0>)\n",
      "epoch  1851 : w =  tensor(1.4319, requires_grad=True)  loss =  tensor(2.4251, grad_fn=<MeanBackward0>)\n",
      "epoch  1861 : w =  tensor(1.4372, requires_grad=True)  loss =  tensor(2.3798, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1871 : w =  tensor(1.4425, requires_grad=True)  loss =  tensor(2.3351, grad_fn=<MeanBackward0>)\n",
      "epoch  1881 : w =  tensor(1.4478, requires_grad=True)  loss =  tensor(2.2911, grad_fn=<MeanBackward0>)\n",
      "epoch  1891 : w =  tensor(1.4531, requires_grad=True)  loss =  tensor(2.2478, grad_fn=<MeanBackward0>)\n",
      "epoch  1901 : w =  tensor(1.4583, requires_grad=True)  loss =  tensor(2.2050, grad_fn=<MeanBackward0>)\n",
      "epoch  1911 : w =  tensor(1.4635, requires_grad=True)  loss =  tensor(2.1630, grad_fn=<MeanBackward0>)\n",
      "epoch  1921 : w =  tensor(1.4687, requires_grad=True)  loss =  tensor(2.1215, grad_fn=<MeanBackward0>)\n",
      "epoch  1931 : w =  tensor(1.4738, requires_grad=True)  loss =  tensor(2.0806, grad_fn=<MeanBackward0>)\n",
      "epoch  1941 : w =  tensor(1.4789, requires_grad=True)  loss =  tensor(2.0404, grad_fn=<MeanBackward0>)\n",
      "epoch  1951 : w =  tensor(1.4840, requires_grad=True)  loss =  tensor(2.0008, grad_fn=<MeanBackward0>)\n",
      "epoch  1961 : w =  tensor(1.4891, requires_grad=True)  loss =  tensor(1.9617, grad_fn=<MeanBackward0>)\n",
      "epoch  1971 : w =  tensor(1.4941, requires_grad=True)  loss =  tensor(1.9233, grad_fn=<MeanBackward0>)\n",
      "epoch  1981 : w =  tensor(1.4991, requires_grad=True)  loss =  tensor(1.8855, grad_fn=<MeanBackward0>)\n",
      "epoch  1991 : w =  tensor(1.5041, requires_grad=True)  loss =  tensor(1.8482, grad_fn=<MeanBackward0>)\n",
      "epoch  2001 : w =  tensor(1.5090, requires_grad=True)  loss =  tensor(1.8115, grad_fn=<MeanBackward0>)\n",
      "epoch  2011 : w =  tensor(1.5140, requires_grad=True)  loss =  tensor(1.7754, grad_fn=<MeanBackward0>)\n",
      "epoch  2021 : w =  tensor(1.5188, requires_grad=True)  loss =  tensor(1.7398, grad_fn=<MeanBackward0>)\n",
      "epoch  2031 : w =  tensor(1.5237, requires_grad=True)  loss =  tensor(1.7048, grad_fn=<MeanBackward0>)\n",
      "epoch  2041 : w =  tensor(1.5286, requires_grad=True)  loss =  tensor(1.6704, grad_fn=<MeanBackward0>)\n",
      "epoch  2051 : w =  tensor(1.5334, requires_grad=True)  loss =  tensor(1.6365, grad_fn=<MeanBackward0>)\n",
      "epoch  2061 : w =  tensor(1.5381, requires_grad=True)  loss =  tensor(1.6031, grad_fn=<MeanBackward0>)\n",
      "epoch  2071 : w =  tensor(1.5429, requires_grad=True)  loss =  tensor(1.5703, grad_fn=<MeanBackward0>)\n",
      "epoch  2081 : w =  tensor(1.5476, requires_grad=True)  loss =  tensor(1.5380, grad_fn=<MeanBackward0>)\n",
      "epoch  2091 : w =  tensor(1.5523, requires_grad=True)  loss =  tensor(1.5062, grad_fn=<MeanBackward0>)\n",
      "epoch  2101 : w =  tensor(1.5570, requires_grad=True)  loss =  tensor(1.4750, grad_fn=<MeanBackward0>)\n",
      "epoch  2111 : w =  tensor(1.5616, requires_grad=True)  loss =  tensor(1.4442, grad_fn=<MeanBackward0>)\n",
      "epoch  2121 : w =  tensor(1.5663, requires_grad=True)  loss =  tensor(1.4140, grad_fn=<MeanBackward0>)\n",
      "epoch  2131 : w =  tensor(1.5708, requires_grad=True)  loss =  tensor(1.3842, grad_fn=<MeanBackward0>)\n",
      "epoch  2141 : w =  tensor(1.5754, requires_grad=True)  loss =  tensor(1.3550, grad_fn=<MeanBackward0>)\n",
      "epoch  2151 : w =  tensor(1.5799, requires_grad=True)  loss =  tensor(1.3262, grad_fn=<MeanBackward0>)\n",
      "epoch  2161 : w =  tensor(1.5844, requires_grad=True)  loss =  tensor(1.2980, grad_fn=<MeanBackward0>)\n",
      "epoch  2171 : w =  tensor(1.5889, requires_grad=True)  loss =  tensor(1.2702, grad_fn=<MeanBackward0>)\n",
      "epoch  2181 : w =  tensor(1.5934, requires_grad=True)  loss =  tensor(1.2428, grad_fn=<MeanBackward0>)\n",
      "epoch  2191 : w =  tensor(1.5978, requires_grad=True)  loss =  tensor(1.2160, grad_fn=<MeanBackward0>)\n",
      "epoch  2201 : w =  tensor(1.6022, requires_grad=True)  loss =  tensor(1.1896, grad_fn=<MeanBackward0>)\n",
      "epoch  2211 : w =  tensor(1.6065, requires_grad=True)  loss =  tensor(1.1636, grad_fn=<MeanBackward0>)\n",
      "epoch  2221 : w =  tensor(1.6109, requires_grad=True)  loss =  tensor(1.1381, grad_fn=<MeanBackward0>)\n",
      "epoch  2231 : w =  tensor(1.6152, requires_grad=True)  loss =  tensor(1.1131, grad_fn=<MeanBackward0>)\n",
      "epoch  2241 : w =  tensor(1.6195, requires_grad=True)  loss =  tensor(1.0885, grad_fn=<MeanBackward0>)\n",
      "epoch  2251 : w =  tensor(1.6237, requires_grad=True)  loss =  tensor(1.0643, grad_fn=<MeanBackward0>)\n",
      "epoch  2261 : w =  tensor(1.6279, requires_grad=True)  loss =  tensor(1.0405, grad_fn=<MeanBackward0>)\n",
      "epoch  2271 : w =  tensor(1.6321, requires_grad=True)  loss =  tensor(1.0172, grad_fn=<MeanBackward0>)\n",
      "epoch  2281 : w =  tensor(1.6363, requires_grad=True)  loss =  tensor(0.9943, grad_fn=<MeanBackward0>)\n",
      "epoch  2291 : w =  tensor(1.6405, requires_grad=True)  loss =  tensor(0.9718, grad_fn=<MeanBackward0>)\n",
      "epoch  2301 : w =  tensor(1.6446, requires_grad=True)  loss =  tensor(0.9497, grad_fn=<MeanBackward0>)\n",
      "epoch  2311 : w =  tensor(1.6487, requires_grad=True)  loss =  tensor(0.9280, grad_fn=<MeanBackward0>)\n",
      "epoch  2321 : w =  tensor(1.6527, requires_grad=True)  loss =  tensor(0.9067, grad_fn=<MeanBackward0>)\n",
      "epoch  2331 : w =  tensor(1.6567, requires_grad=True)  loss =  tensor(0.8858, grad_fn=<MeanBackward0>)\n",
      "epoch  2341 : w =  tensor(1.6607, requires_grad=True)  loss =  tensor(0.8653, grad_fn=<MeanBackward0>)\n",
      "epoch  2351 : w =  tensor(1.6647, requires_grad=True)  loss =  tensor(0.8451, grad_fn=<MeanBackward0>)\n",
      "epoch  2361 : w =  tensor(1.6687, requires_grad=True)  loss =  tensor(0.8254, grad_fn=<MeanBackward0>)\n",
      "epoch  2371 : w =  tensor(1.6726, requires_grad=True)  loss =  tensor(0.8060, grad_fn=<MeanBackward0>)\n",
      "epoch  2381 : w =  tensor(1.6765, requires_grad=True)  loss =  tensor(0.7870, grad_fn=<MeanBackward0>)\n",
      "epoch  2391 : w =  tensor(1.6803, requires_grad=True)  loss =  tensor(0.7683, grad_fn=<MeanBackward0>)\n",
      "epoch  2401 : w =  tensor(1.6841, requires_grad=True)  loss =  tensor(0.7500, grad_fn=<MeanBackward0>)\n",
      "epoch  2411 : w =  tensor(1.6880, requires_grad=True)  loss =  tensor(0.7321, grad_fn=<MeanBackward0>)\n",
      "epoch  2421 : w =  tensor(1.6917, requires_grad=True)  loss =  tensor(0.7145, grad_fn=<MeanBackward0>)\n",
      "epoch  2431 : w =  tensor(1.6955, requires_grad=True)  loss =  tensor(0.6972, grad_fn=<MeanBackward0>)\n",
      "epoch  2441 : w =  tensor(1.6992, requires_grad=True)  loss =  tensor(0.6803, grad_fn=<MeanBackward0>)\n",
      "epoch  2451 : w =  tensor(1.7029, requires_grad=True)  loss =  tensor(0.6637, grad_fn=<MeanBackward0>)\n",
      "epoch  2461 : w =  tensor(1.7066, requires_grad=True)  loss =  tensor(0.6474, grad_fn=<MeanBackward0>)\n",
      "epoch  2471 : w =  tensor(1.7102, requires_grad=True)  loss =  tensor(0.6315, grad_fn=<MeanBackward0>)\n",
      "epoch  2481 : w =  tensor(1.7138, requires_grad=True)  loss =  tensor(0.6158, grad_fn=<MeanBackward0>)\n",
      "epoch  2491 : w =  tensor(1.7174, requires_grad=True)  loss =  tensor(0.6005, grad_fn=<MeanBackward0>)\n",
      "epoch  2501 : w =  tensor(1.7209, requires_grad=True)  loss =  tensor(0.5855, grad_fn=<MeanBackward0>)\n",
      "epoch  2511 : w =  tensor(1.7245, requires_grad=True)  loss =  tensor(0.5708, grad_fn=<MeanBackward0>)\n",
      "epoch  2521 : w =  tensor(1.7280, requires_grad=True)  loss =  tensor(0.5564, grad_fn=<MeanBackward0>)\n",
      "epoch  2531 : w =  tensor(1.7314, requires_grad=True)  loss =  tensor(0.5423, grad_fn=<MeanBackward0>)\n",
      "epoch  2541 : w =  tensor(1.7349, requires_grad=True)  loss =  tensor(0.5285, grad_fn=<MeanBackward0>)\n",
      "epoch  2551 : w =  tensor(1.7383, requires_grad=True)  loss =  tensor(0.5150, grad_fn=<MeanBackward0>)\n",
      "epoch  2561 : w =  tensor(1.7417, requires_grad=True)  loss =  tensor(0.5018, grad_fn=<MeanBackward0>)\n",
      "epoch  2571 : w =  tensor(1.7450, requires_grad=True)  loss =  tensor(0.4888, grad_fn=<MeanBackward0>)\n",
      "epoch  2581 : w =  tensor(1.7484, requires_grad=True)  loss =  tensor(0.4761, grad_fn=<MeanBackward0>)\n",
      "epoch  2591 : w =  tensor(1.7517, requires_grad=True)  loss =  tensor(0.4637, grad_fn=<MeanBackward0>)\n",
      "epoch  2601 : w =  tensor(1.7550, requires_grad=True)  loss =  tensor(0.4516, grad_fn=<MeanBackward0>)\n",
      "epoch  2611 : w =  tensor(1.7582, requires_grad=True)  loss =  tensor(0.4397, grad_fn=<MeanBackward0>)\n",
      "epoch  2621 : w =  tensor(1.7614, requires_grad=True)  loss =  tensor(0.4280, grad_fn=<MeanBackward0>)\n",
      "epoch  2631 : w =  tensor(1.7646, requires_grad=True)  loss =  tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "epoch  2641 : w =  tensor(1.7678, requires_grad=True)  loss =  tensor(0.4055, grad_fn=<MeanBackward0>)\n",
      "epoch  2651 : w =  tensor(1.7709, requires_grad=True)  loss =  tensor(0.3946, grad_fn=<MeanBackward0>)\n",
      "epoch  2661 : w =  tensor(1.7740, requires_grad=True)  loss =  tensor(0.3840, grad_fn=<MeanBackward0>)\n",
      "epoch  2671 : w =  tensor(1.7771, requires_grad=True)  loss =  tensor(0.3736, grad_fn=<MeanBackward0>)\n",
      "epoch  2681 : w =  tensor(1.7802, requires_grad=True)  loss =  tensor(0.3634, grad_fn=<MeanBackward0>)\n",
      "epoch  2691 : w =  tensor(1.7832, requires_grad=True)  loss =  tensor(0.3534, grad_fn=<MeanBackward0>)\n",
      "epoch  2701 : w =  tensor(1.7862, requires_grad=True)  loss =  tensor(0.3437, grad_fn=<MeanBackward0>)\n",
      "epoch  2711 : w =  tensor(1.7892, requires_grad=True)  loss =  tensor(0.3342, grad_fn=<MeanBackward0>)\n",
      "epoch  2721 : w =  tensor(1.7922, requires_grad=True)  loss =  tensor(0.3249, grad_fn=<MeanBackward0>)\n",
      "epoch  2731 : w =  tensor(1.7951, requires_grad=True)  loss =  tensor(0.3158, grad_fn=<MeanBackward0>)\n",
      "epoch  2741 : w =  tensor(1.7980, requires_grad=True)  loss =  tensor(0.3070, grad_fn=<MeanBackward0>)\n",
      "epoch  2751 : w =  tensor(1.8008, requires_grad=True)  loss =  tensor(0.2983, grad_fn=<MeanBackward0>)\n",
      "epoch  2761 : w =  tensor(1.8037, requires_grad=True)  loss =  tensor(0.2899, grad_fn=<MeanBackward0>)\n",
      "epoch  2771 : w =  tensor(1.8065, requires_grad=True)  loss =  tensor(0.2816, grad_fn=<MeanBackward0>)\n",
      "epoch  2781 : w =  tensor(1.8093, requires_grad=True)  loss =  tensor(0.2736, grad_fn=<MeanBackward0>)\n",
      "epoch  2791 : w =  tensor(1.8121, requires_grad=True)  loss =  tensor(0.2657, grad_fn=<MeanBackward0>)\n",
      "epoch  2801 : w =  tensor(1.8148, requires_grad=True)  loss =  tensor(0.2580, grad_fn=<MeanBackward0>)\n",
      "epoch  2811 : w =  tensor(1.8175, requires_grad=True)  loss =  tensor(0.2505, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  2821 : w =  tensor(1.8202, requires_grad=True)  loss =  tensor(0.2432, grad_fn=<MeanBackward0>)\n",
      "epoch  2831 : w =  tensor(1.8228, requires_grad=True)  loss =  tensor(0.2361, grad_fn=<MeanBackward0>)\n",
      "epoch  2841 : w =  tensor(1.8255, requires_grad=True)  loss =  tensor(0.2291, grad_fn=<MeanBackward0>)\n",
      "epoch  2851 : w =  tensor(1.8281, requires_grad=True)  loss =  tensor(0.2223, grad_fn=<MeanBackward0>)\n",
      "epoch  2861 : w =  tensor(1.8307, requires_grad=True)  loss =  tensor(0.2157, grad_fn=<MeanBackward0>)\n",
      "epoch  2871 : w =  tensor(1.8332, requires_grad=True)  loss =  tensor(0.2093, grad_fn=<MeanBackward0>)\n",
      "epoch  2881 : w =  tensor(1.8357, requires_grad=True)  loss =  tensor(0.2030, grad_fn=<MeanBackward0>)\n",
      "epoch  2891 : w =  tensor(1.8382, requires_grad=True)  loss =  tensor(0.1968, grad_fn=<MeanBackward0>)\n",
      "epoch  2901 : w =  tensor(1.8407, requires_grad=True)  loss =  tensor(0.1909, grad_fn=<MeanBackward0>)\n",
      "epoch  2911 : w =  tensor(1.8432, requires_grad=True)  loss =  tensor(0.1850, grad_fn=<MeanBackward0>)\n",
      "epoch  2921 : w =  tensor(1.8456, requires_grad=True)  loss =  tensor(0.1794, grad_fn=<MeanBackward0>)\n",
      "epoch  2931 : w =  tensor(1.8480, requires_grad=True)  loss =  tensor(0.1738, grad_fn=<MeanBackward0>)\n",
      "epoch  2941 : w =  tensor(1.8504, requires_grad=True)  loss =  tensor(0.1684, grad_fn=<MeanBackward0>)\n",
      "epoch  2951 : w =  tensor(1.8527, requires_grad=True)  loss =  tensor(0.1632, grad_fn=<MeanBackward0>)\n",
      "epoch  2961 : w =  tensor(1.8550, requires_grad=True)  loss =  tensor(0.1581, grad_fn=<MeanBackward0>)\n",
      "epoch  2971 : w =  tensor(1.8573, requires_grad=True)  loss =  tensor(0.1531, grad_fn=<MeanBackward0>)\n",
      "epoch  2981 : w =  tensor(1.8596, requires_grad=True)  loss =  tensor(0.1483, grad_fn=<MeanBackward0>)\n",
      "epoch  2991 : w =  tensor(1.8619, requires_grad=True)  loss =  tensor(0.1436, grad_fn=<MeanBackward0>)\n",
      "Prediction after training: f(5) = 7.455\n",
      "tensor([1.8637, 3.7273, 5.5910, 7.4546], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(4).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.001\n",
    "n_iters = 3000\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([w], lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_predicted = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(4).item():.3f}')\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-nerve",
   "metadata": {},
   "source": [
    "### model loss and optimizer steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "determined-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = -4.734\n",
      "epoch  1 : w =  -0.3842853903770447  loss =  tensor(68.4450, grad_fn=<MeanBackward0>)\n",
      "epoch  11 : w =  1.5137978792190552  loss =  tensor(1.7922, grad_fn=<MeanBackward0>)\n",
      "epoch  21 : w =  1.8221360445022583  loss =  tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "epoch  31 : w =  1.8746757507324219  loss =  tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "epoch  41 : w =  1.8859834671020508  loss =  tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "epoch  51 : w =  1.8905748128890991  loss =  tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "epoch  61 : w =  1.894003987312317  loss =  tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "epoch  71 : w =  1.8971667289733887  loss =  tensor(0.0154, grad_fn=<MeanBackward0>)\n",
      "epoch  81 : w =  1.9002091884613037  loss =  tensor(0.0145, grad_fn=<MeanBackward0>)\n",
      "epoch  91 : w =  1.9031578302383423  loss =  tensor(0.0136, grad_fn=<MeanBackward0>)\n",
      "Prediction after training: f(5) = 9.806\n",
      "tensor([[2.1834],\n",
      "        [4.0889],\n",
      "        [5.9943],\n",
      "        [7.8998]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-blackjack",
   "metadata": {},
   "source": [
    "## Linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "early-litigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 4154.7261\n",
      "epoch: 20, loss = 2926.1082\n",
      "epoch: 30, loss = 2088.4529\n",
      "epoch: 40, loss = 1517.2306\n",
      "epoch: 50, loss = 1127.6168\n",
      "epoch: 60, loss = 861.8191\n",
      "epoch: 70, loss = 680.4537\n",
      "epoch: 80, loss = 556.6767\n",
      "epoch: 90, loss = 472.1857\n",
      "epoch: 100, loss = 414.5013\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "# cast to float Tensor\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size) \n",
    "\n",
    "# 2) Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# 3) Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# Plot\n",
    "predicted = model(X).detach().numpy()\n",
    "\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-clock",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "daily-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss = 0.2266\n",
      "epoch: 200, loss = 0.1663\n",
      "epoch: 300, loss = 0.1390\n",
      "epoch: 400, loss = 0.1227\n",
      "epoch: 500, loss = 0.1117\n",
      "epoch: 600, loss = 0.1036\n",
      "epoch: 700, loss = 0.0974\n",
      "epoch: 800, loss = 0.0925\n",
      "epoch: 900, loss = 0.0884\n",
      "epoch: 1000, loss = 0.0850\n",
      "accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) Prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b , sigmoid at the end\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Model(n_features)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-nation",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "DataLoader can do the batch computation for us\n",
    "\n",
    "#### To implement custom dataset\n",
    "\n",
    "* need to inherit dataset\n",
    "* implement \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "middle-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#df1 = pd.read_csv(\"wine.data\")\n",
    "#df1.to_csv(\"wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "connected-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        xy = np.loadtxt('wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n",
    "        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "miniature-pencil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3200e+01, 1.7800e+00, 2.1400e+00, 1.1200e+01, 1.0000e+02, 2.6500e+00,\n",
      "        2.7600e+00, 2.6000e-01, 1.2800e+00, 4.3800e+00, 1.0500e+00, 3.4000e+00,\n",
      "        1.0500e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# get first sample and unpack\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "victorian-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whole dataset with DataLoader\n",
    "# shuffle: shuffle data, good for training\n",
    "# num_workers: faster loading with multiple subprocesses\n",
    "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "authentic-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3860e+01, 1.5100e+00, 2.6700e+00, 2.5000e+01, 8.6000e+01, 2.9500e+00,\n",
      "         2.8600e+00, 2.1000e-01, 1.8700e+00, 3.3800e+00, 1.3600e+00, 3.1600e+00,\n",
      "         4.1000e+02],\n",
      "        [1.3030e+01, 9.0000e-01, 1.7100e+00, 1.6000e+01, 8.6000e+01, 1.9500e+00,\n",
      "         2.0300e+00, 2.4000e-01, 1.4600e+00, 4.6000e+00, 1.1900e+00, 2.4800e+00,\n",
      "         3.9200e+02],\n",
      "        [1.4830e+01, 1.6400e+00, 2.1700e+00, 1.4000e+01, 9.7000e+01, 2.8000e+00,\n",
      "         2.9800e+00, 2.9000e-01, 1.9800e+00, 5.2000e+00, 1.0800e+00, 2.8500e+00,\n",
      "         1.0450e+03],\n",
      "        [1.2080e+01, 1.1300e+00, 2.5100e+00, 2.4000e+01, 7.8000e+01, 2.0000e+00,\n",
      "         1.5800e+00, 4.0000e-01, 1.4000e+00, 2.2000e+00, 1.3100e+00, 2.7200e+00,\n",
      "         6.3000e+02]]) tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "# convert to an iterator and look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "genuine-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 45\n",
      "Epoch: 1/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 45/45| Inputs torch.Size([1, 13]) | Labels torch.Size([1, 1])\n",
      "Epoch: 2/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 45/45| Inputs torch.Size([1, 13]) | Labels torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Dummy Training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    #The enumerate() function assigns an index to each item in an iterable object \n",
    "    #that can be used to reference the item later\n",
    "    # trainloader is a tuple returns inputs, labels, and i \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "architectural-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some famous datasets are available in torchvision.datasets\n",
    "# e.g. MNIST, Fashion-MNIST, CIFAR10, COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=torchvision.transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=3, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "inputs, targets = data\n",
    "print(inputs.shape, targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-frequency",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
    "during creation of the DataSet\n",
    "\n",
    "complete list of built-in transforms: \n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "On Images\n",
    "\n",
    "CenterCrop, Grayscale, Pad, RandomAffine\n",
    "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
    "Resize, Scale\n",
    "\n",
    "On Tensors\n",
    "LinearTransformation, Normalize, RandomErasing\n",
    "\n",
    "Conversion\n",
    "ToPILImage: from tensor or ndrarray\n",
    "ToTensor : from numpy.ndarray or PILImage\n",
    "\n",
    "Generic\n",
    "Use Lambda \n",
    "\n",
    "Custom\n",
    "Write own class\n",
    "\n",
    "Compose multiple Transforms:\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                               RandomCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "differential-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Transform\n",
      "\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[1.32e+01 1.78e+00 2.14e+00 1.12e+01 1.00e+02 2.65e+00 2.76e+00 2.60e-01\n",
      " 1.28e+00 4.38e+00 1.05e+00 3.40e+00 1.05e+03] [1.]\n",
      "\n",
      "With Tensor Transform\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([1.3200e+01, 1.7800e+00, 2.1400e+00, 1.1200e+01, 1.0000e+02, 2.6500e+00,\n",
      "        2.7600e+00, 2.6000e-01, 1.2800e+00, 4.3800e+00, 1.0500e+00, 3.4000e+00,\n",
      "        1.0500e+03]) tensor([1.])\n",
      "\n",
      "With Tensor and Multiplication Transform\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([5.2800e+01, 7.1200e+00, 8.5600e+00, 4.4800e+01, 4.0000e+02, 1.0600e+01,\n",
      "        1.1040e+01, 1.0400e+00, 5.1200e+00, 1.7520e+01, 4.2000e+00, 1.3600e+01,\n",
      "        4.2000e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        xy = np.loadtxt('wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        # note that we do not convert to tensor here\n",
    "        self.x_data = xy[:, 1:]\n",
    "        self.y_data = xy[:, [0]]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Custom Transforms\n",
    "# implement __call__(self, sample)\n",
    "class ToTensor:\n",
    "    # Convert ndarrays to Tensors\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "class MulTransform:\n",
    "    # multiply inputs with a given factor\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n",
    "\n",
    "print('Without Transform')\n",
    "dataset = WineDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(features, labels)\n",
    "\n",
    "print('\\nWith Tensor Transform')\n",
    "dataset = WineDataset(transform=ToTensor())\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(features, labels)\n",
    "\n",
    "print('\\nWith Tensor and Multiplication Transform')\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-acceptance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-watershed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
